## Vocabulary parameters ##
vocab_size=128000
subsample_size=51200000
num_placeholders=256
## BERT training parameters ##
train_batch_size=128
max_predictions=20
max_seq_length=512
masked_lm_prob=0.15
eval_batch_size=64
learning_rate=2e-5
train_steps=1000000
save_checkpoint_steps=2500
save_summary_steps=1000
num_tpu_cores=8
iterations_per_loop=1000
predict_batch_size=64
num_train_epochs=3.0
warmup_proportion=0.1
## Tokenization should lower case for uncased ##
do_lowercase=True